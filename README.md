# DMT2023 Homework 3 - Data Science Project
## Overview
This project demonstrates the application of Dimensionality Reduction and Supervised Learning techniques in processing and classifying text data. Using the Le Morte d'Arthur dataset, the project aims to showcase text preprocessing, topic modeling, and machine learning to create a classification pipeline. The key focus is on extracting meaningful insights from a large dataset, optimizing machine learning models, and evaluating their performance.
  
## Project Highlights
1)Dimensionality Reduction: Implemented Latent Semantic Analysis (LSA) to identify key topics in the dataset, using topic modeling to understand underlying themes.  
2)Text Preprocessing: Performed extensive text cleaning, tokenization, stemming, and stop-word removal to prepare data for modeling.  
3) Machine Learning Classification: Developed and optimized a Random Forest Classifier pipeline with TF-IDF encoding to classify chapters from Le Morte d'Arthur into two categories.  
4) Hyperparameter Tuning: Utilized GridSearchCV to optimize the classifier and improve its accuracy on a validation set.  
5) Model Evaluation: Evaluated the model's performance using confusion matrices and cross-validation, ensuring the best possible classification accuracy.  
  
## Key Features
1) Text Processing: Extracted chapter titles, cleaned and preprocessed text, removed noise, and reduced dimensionality using LSA.  
2) Classification Pipeline: Built a robust pipeline combining TF-IDF for text vectorization and Random Forest for classification.  
3) Optimization: Performed hyperparameter tuning on key parameters such as n_estimators, max_depth, and max_features to maximize model performance.  
4) Evaluation: Used various metrics, including cross-validation and confusion matrices, to assess the model's ability to classify the data.
   
## Skills and Technologies Used
1) Python: Core language for text processing, machine learning, and model evaluation.  
2) Scikit-learn: For creating machine learning pipelines, performing grid search, and model evaluation.  
3) Gensim: For topic modeling using Latent Semantic Analysis (LSA).  
4) NLTK: For text preprocessing, including tokenization, stemming, and stop-word removal.  
5) Pandas: For data manipulation and cleaning.  
6) Matplotlib: For data visualization, including plotting performance metrics.  
7) Time Management: Ensured that models were optimized within a limited time frame as per project constraints.  

## Outcome
1) Successfully classified chapters from Le Morte d'Arthur into two books using a Random Forest Classifier.  
2) Identified the optimal number of topics for Latent Semantic Analysis using coherence scores.  
3) Achieved optimal classification performance by fine-tuning model hyperparameters.  
This project highlights my ability to apply machine learning and text processing techniques to real-world data. It showcases my skills in dimensionality reduction, model optimization, and evaluation, making me a strong candidate for roles in data science, machine learning, and natural language processing.
